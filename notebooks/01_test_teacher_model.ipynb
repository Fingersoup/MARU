{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Teacher Model (Ollama)\n",
    "\n",
    "This notebook tests the connection to the local Ollama server and verifies the teacher model (gemma3-27b-it-qat:latest) is working correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import time\n",
    "import json\n",
    "from typing import Dict, Any"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Basic Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ollama_connection():\n",
    "    \"\"\"Test basic connection to Ollama server\"\"\"\n",
    "    try:\n",
    "        # List available models\n",
    "        models = ollama.list()\n",
    "        print(\"Available models:\")\n",
    "        for model in models['models']:\n",
    "            print(f\"  - {model['name']}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to Ollama: {e}\")\n",
    "        return False\n",
    "\n",
    "# Test connection\n",
    "connection_ok = test_ollama_connection()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Teacher Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEACHER_MODEL = \"gemma3-27b-it-qat:latest\"\n",
    "\n",
    "def query_teacher_model(prompt: str, model: str = TEACHER_MODEL) -> Dict[str, Any]:\n",
    "    \"\"\"Query the teacher model with error handling\"\"\"\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = ollama.generate(\n",
    "            model=model,\n",
    "            prompt=prompt,\n",
    "            options={\n",
    "                'temperature': 0.1,  # Low temperature for consistent responses\n",
    "                'top_p': 0.9,\n",
    "                'num_predict': 100  # Limit response length for testing\n",
    "            }\n",
    "        )\n",
    "        end_time = time.time()\n",
    "        \n",
    "        return {\n",
    "            'success': True,\n",
    "            'response': response['response'],\n",
    "            'duration': end_time - start_time,\n",
    "            'model': model\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'success': False,\n",
    "            'error': str(e),\n",
    "            'model': model\n",
    "        }\n",
    "\n",
    "# Test simple query\n",
    "test_prompt = \"What is 2 + 2? Please answer with just the number.\"\n",
    "result = query_teacher_model(test_prompt)\n",
    "\n",
    "print(f\"Query result: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Needle-in-Haystack Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_needle_task():\n",
    "    \"\"\"Test the teacher model on a simple needle-in-haystack task\"\"\"\n",
    "    \n",
    "    # Create a simple test case\n",
    "    haystack = \"The quick brown fox jumps over the lazy dog. NEEDLE_STRING is here. More random text follows.\"\n",
    "    needle = \"NEEDLE_STRING\"\n",
    "    \n",
    "    prompt = f\"\"\"Find the starting position (character index) of the string \"{needle}\" in the following text:\n",
    "\n",
    "{haystack}\n",
    "\n",
    "Please respond with just the number (the character index where \"{needle}\" starts).\"\"\"\n",
    "    \n",
    "    result = query_teacher_model(prompt)\n",
    "    \n",
    "    if result['success']:\n",
    "        print(f\"Teacher response: {result['response']}\")\n",
    "        print(f\"Duration: {result['duration']:.2f} seconds\")\n",
    "        \n",
    "        # Verify the correct answer\n",
    "        correct_position = haystack.find(needle)\n",
    "        print(f\"Correct position: {correct_position}\")\n",
    "    else:\n",
    "        print(f\"Error: {result['error']}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Run the test\n",
    "needle_result = test_needle_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook verifies:\n",
    "1. Connection to Ollama server\n",
    "2. Teacher model availability and response quality\n",
    "3. Basic needle-in-haystack task performance\n",
    "\n",
    "The teacher model will be used to generate training data for the MARU architecture."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
